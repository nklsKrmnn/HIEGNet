use_gpu: true
run_name: "eval_mlp_gs16_features_between"
model_parameters:
  mlp:
    mlp_hidden_dim: 256
    mlp_hidden_layers: 16
    output_dim: 3
    mlp_dropout: 0.0
    norm: none
    mlp_softmax_function: "softmax" # Options: "softmax", "log_softmax", "none" (Default: "softmax")
training_parameters:
  batch_size: 1
  epochs: 5000
  learning_rate: 0.0001
  #lr_scheduler_params:
  #  scheduler: "OneCycleLR" # Options: "ReduceLROnPlateau", "CyclicLR", "OneCycleLR"
  #  params: # Give the parameters for the specific class you chose. Here you can find the documentation: https://pytorch.org/docs/stable/optim.html
  #    max_lr: 0.003
  loss: "crossentropy" # Options: "mse", "crossentropy", "nll", "weighted_mse"
  optimizer: "adam" # Options: "adam", "sgd"
  weight_decay: 0.000000
  balance_classes: true
  momentum: 0.1
  seed: 69
  batch_shuffle: false
  patience: 100000000
  log_image_frequency: 100
  n_folds: 0 # Set to 0 for no cross-validation
  n_test_initialisations: 20
  reported_set: "test"
dataset_parameters:
  class_name: glom_graph_dataset
  root: "/data/4_input_data_graphs/glob_graphs/"
  feature_file_path: "/data/3_extracted_features/EXC/glom_features_uniform.csv"
  annotations_path: "/data/1_cytomine_downloads/EXC/annotations/25/"
  set_indices_path: "/repos/histograph/data/input/set_indices/test15_val15"
  split_action: "load" # Options: "load", "save", "split"
  validation_split: 0.0
  test_split: 1.0
  processed_file_name: "glom_graph"
  glom_graph:
    method: "knn" # Options: "knn", "radius", "delaunay"
    k: 0 # Options: "k", "radius", ""
  train_patients:
    - 001
    - 002
    - 003
    #- 004
  test_patients:
    #- 001
    #- 002
    #- 003
    - 004
  random_seed: 123
  onehot_targets: true
  process: true
  preprocessing_params:
    scaler: 'MinMaxScaler'
  feature_list:
    - 'Area (micronsÂ²)'
    - 'Perimeter (mm)'
    - 'circularity'
    - 'aspect_ratio'
    - 'eccentricity'
    - 'lbp_uniform_0'
    - 'lbp_uniform_1'
    - 'lbp_uniform_2'
    - 'lbp_uniform_3'
    - 'lbp_uniform_4'
    - 'lbp_uniform_5'
    - 'lbp_uniform_6'
    - 'lbp_uniform_7'
    #- 'lbp_uniform_8'
    - 'lbp_uniform_9'


