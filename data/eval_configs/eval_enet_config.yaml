use_gpu: true
run_name: "eval_efficientnetV2m_p1-3_between"
model_parameters:
  efficientnet_v2_pretrained:
    enet_size: m
    output_dim: 3
    dropout: 0.1
training_parameters:
  batch_size: 14
  epochs: 300
  learning_rate: 0.00001 # Will be ignored with OneCycleLR schduler
  lr_scheduler_params:
    scheduler: "OneCycleLR" # Options: "ReduceLROnPlateau", "CyclicLR", "OneCycleLR"
    params: # Give the parameters for the specific class you chose. Here you can find the documentation: https://pytorch.org/docs/stable/optim.html
      max_lr: 0.0001
  loss: "crossentropy" # Options: "mse", "crossentropy", "nll"
  optimizer: "adam" # Options: "adam", "sgd"
  weight_decay: 0.000000
  balance_classes: true
  momentum: 0.1
  seed: 69
  batch_shuffle: false
  patience: 100000000
  log_image_frequency: 50
  n_test_initialisations: 20
  n_folds: 0
  reported_set: "test"
dataset_parameters:
  class_name: image_dataset
  validation_split: 0.0
  test_split: 1.0
  process: true
  norm_mean: 0.6532708406448364
  norm_std: 0.19844505190849304
  set_indices_path: "/repos/histograph/data/input/set_indices/test15_val15"
  split_action: "load" # Options: "load", "save", "split"
  train_patients:
    - 001
    - 002
    - 003
  test_patients:
    #- 001
    #- 002
    #- 003
    - 004
  random_seed: 123
  onehot_targets: true
  hot_load: false
  image_file_path: "/data/3_extracted_features/EXC/image_paths_p1-4.csv"
  annotations_path: "/data/1_cytomine_downloads/EXC/annotations/25/"
  feature_list:
    - 'path_25'
    #- 'path_106'
    #- 'path_107'
    #- 'path_108'
