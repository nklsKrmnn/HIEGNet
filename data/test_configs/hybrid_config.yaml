use_gpu: true
run_name: "coding"
model_parameters:
  hybrid:
    hidden_channels: [20, 30, 40, 50] #output dim: 361
    kernel_size: 4
    input_channels: 6
    stride: 2
    gnn_params:
      class_name: gat_v2
      input_dim: 150
      hidden_dims: [80,80]
      n_fc_layers: 2
      norm_fc_layers: batch
      output_dim: 3
      dropout: 0.7
training_parameters:
  batch_size: 1
  epochs: 10000
  learning_rate: 0.0003
  lr_scheduler_params:
    scheduler: "OneCycleLR" # Options: "ReduceLROnPlateau", "CyclicLR", "OneCycleLR"
    params: # Give the parameters for the specific class you chose. Here you can find the documentation: https://pytorch.org/docs/stable/optim.html
      max_lr: 0.003
  loss: "crossentropy" # Options: "mse", "crossentropy", "nll"
  optimizer: "adam" # Options: "adam", "sgd"
  weight_decay: 0.000
  balance_classes: true
  momentum: 0.1
  seed: 69
  batch_shuffle: false
  patience: 10000
  log_image_frequency: 20
  n_folds: 0
dataset_parameters:
  class_name: glom_graph_dataset
  #root: "/home/niklas/drive/4_input_data_graphs/"
  root: "/home/dascim/data/4_input_data_graphs/glob_graphs/"
  #feature_file_path: "/home/niklas/drive/4_input_data_graphs/cell_features.csv"
  feature_file_path: "/home/dascim/data/3_extracted_features/EXC/image_paths.csv"
  #annotations_path: "/home/niklas/drive/4_input_data_graphs/annotations/"
  annotations_path: "/home/dascim/data/1_cytomine_downloads/EXC/annotations/25/"
  validation_split: 0.25
  processed_file_name: "glom_graph"
  n_neighbours: 5
  train_patients:
    - 001
    - 003
  random_seed: 123
  onehot_targets: true
  process: true
  preprocessing_params:
    scaler: 'MinMaxScaler'
  #path_image_inputs: "/home/niklas/drive/2_images_preprocessed/25/"
  path_image_inputs: "/home/dascim/data/2_images_preprocessed/EXC/patches_low_resolution/"
  feature_list:
    - 'path_25'
    - 'path_106'
